\section{Abstract}%
\label{sec:abstract}

Gradient descent is an algorithm to iteratively optimize (by either minimizing or maximizing) a continuous function. It does so by shifting the value of the paramaters in the direction of its gradient with respect to the function. In the context of minimization, it is defined as follows
\begin{align*}
         \theta'=\theta-\alpha \frac{d}{d\theta}L(\theta)
\end{align*}
where $\theta$ is the paramater to optimize to minimize the value of the function $L$, and $\alpha$ is a scaling factor. This equation can be generalized to optimize many paramaters by utilizing partial derivatives instead. Anyhow, it is obvious why a continuous function is required: gradients are only defined with continuous functions.

The main goal of this essay is to test whether a discrete loss function and its corresponding discrete paramaters can be generalized to a continuous loss function and continuous paramaters, with the purpose of finding optimum discrete variables. But what fun is it, without applications? So to make it more interesting, I chose to apply it to a programming contest by Al Zimmermann,\cite{zimmermann} which had rendered analytical solutions to the problem impossible. The word ``programming'' within the contest name indicates that the problem was meant to be approached with algorithms that do not utilize gradients, such as hill climbing and simulated annealing\footnote{Which are beyond the scope of this essay}. But I wanted to do it with gradients! Because that's what my calculus classes are for.

The objective of Al Zimmermann's programming contest, is to rearrange discrete tokens within a discrete grid, to minimize the the a loss function which depends on the distances between these tokens on a toroidal surface. The intricacies of this problem are explained within the essay.

There are many techniques used within this essay to accomplish this goal. Among them are
\begin{enumerate}
  \item Generalization of Euclidean distances to accomodate toroidal surfaces
  \item Matric permutations to remove duplicate entries
  \item ``Superposition'' to model discrete tokens being within multiple positions, with their respective ``probabilities''.\footnote{Probabilities here does not refer to the chance of a random event occuring, but rather the ``confidence'' in which a token is in its position, or the extent in which the position affects the loss function}
  \item Enforcement of probabilities (that they should sum to one), using the Sinkhorn-Knopp algorithm.\cite{sinkhorn1967concerning}
  \item Use of Jacobian matrices for gradient descent
\end{enumerate}
