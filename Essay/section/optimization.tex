\section{Optimization}%
\label{sec:optimization}

\subsection{Gradient Descent}%
\label{sub:gradient_descent}
Gradient descent is an iterative optimization algorithm, utilizing the first derivative of the loss function $L$ with respect to all function parameters $\theta$. To recall, a single iteration of gradient descent is as follows:
\begin{align}
         \theta'=\theta-\alpha \frac{\delta}{\delta \theta}L(\theta)
\end{align}
$\alpha$ is the \emph{learning rate}, an arbitrary positive scaling factor, determining the magnitude of the update \wrt{} the gradient.

\begin{figure}[htpb]
        \centering
        \input{fig/gradient.tex}
        \caption{Demonstration of gradient descent convergence: 10 iterations with $\alpha =5\times 10^{-2}$ and $\theta_0=1.8$.}
        \label{fig:gradient_demo}
\end{figure}

\autoref{fig:gradient_demo} is a toy example of gradient descent, but it generalizes to more complicated problems such as Reversing Nearness.

\subsubsection{Partial Derivative of Loss Function}%
\label{ssub:derivative_of_loss_function}
A partial derivative is the derivative of a multi-variable function \wrt{} a single variable, and is denoted by $\frac{\delta}{\delta x}$ instead of $\frac{d}{dx}$. Hence, our goal is to find the Jacobian matrix $\bm{J}$ of $L(S)$ \wrt{} $S$, defined as follows:
 \begin{align}
     \bm{J}=\frac{\delta L(S)}{\delta S}= \begin{bmatrix}
                 \frac{\delta L(S)}{\delta S_{1,1}}&\cdots &\frac{\delta L(S)}{\delta S_{1,N^2}}\\
                 \vdots &\ddots &\vdots \\
                 \frac{\delta L(S)}{\delta S_{N^2,1}}&\cdots &\frac{\delta L(S)}{\delta S_{N^2,N^2}}\\
         \end{bmatrix}
         \label{eq:jacobian_definition}
\end{align}

To do that, a general solution to the partial derivative: $ \frac{\delta L(S)}{\delta S_{i,j}}$, is required. Recall that the loss function is defined as
\begin{align}
    L(S)=\frac{1}{2}\sum_{a}^{N^2} \sum_{b}^{N^2} \sum_{c}^{N^2} \sum_{d}^{N^2} S_{a,b} S_{c,d} C(O)_{a,c}C(O)_{b,d}-c
\end{align}
When evaluating $\bm{J}_{i,j}$, only $S_{i,j}$ is treated as a variable, whereas others are treated as constants, evaluating to $0$ after differentiation. Note that $S_{i,j}$ is only included within the loss when $(a,b)=(i,j)$ or $(c,d)=(i,j)$ or both. The derivatives of the terms of for the respective cases are as follows:
\begin{align}
    \bm{A}=\frac{\delta L(S)}{\delta S_{i,j}}&=\frac{\delta}{\delta S_{i,j}}\left(\frac{1}{2}\sum_{c}^{N^2} \sum_{d}^{N^2} S_{\highlight{i,j}} S_{c,d} C(O)_{\highlight{i},c}C(O)_{\highlight{j},d}\right)&\text{first case}\nonumber\\
              &=\frac{1}{2}\sum_{c}^{N^2} \sum_{d}^{N^2} S_{c,d} C(O)_{i,c}C(O)_{j,d}&\nonumber\\
    \bm{B}=\frac{\delta L(S)}{\delta S_{i,j}}&=\frac{\delta}{\delta S_{i,j}}\left(\frac{1}{2}\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} S_{\highlight{i,j}} C(O)_{a,\highlight{i}}C(O)_{b,\highlight{j}}\right)&\text{second case}\nonumber\\
              &=\frac{1}{2}\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{a,i}C(O)_{b,j}&\nonumber\\
    \bm{C}=\frac{\delta L(S)}{\delta S_{i,j}}&=\frac{\delta}{\delta S_{i,j}}\left(\frac{1}{2}S_{\highlight{i,j}} S_{\highlight{i,j}} C(O)_{\highlight{i,i}}C(O)_{\highlight{j,j}}\right)&\text{third case}\nonumber\\
              &=S_{i,j} C(O)_{i,i}C(O)_{j,j}&\nonumber\\
              &=0&
\end{align}
$\bm{C}$ is $0$ due because it includes the distance of a token against itself. Therefore, $\frac{\delta L(S)}{\delta S_{i,j}}$, taking into account all 3 cases, is equal to $\bm{A}+\bm{B}-\bm{C}=\bm{A}+\bm{B}$, but it can be simplified,
\begin{align}
    \frac{\delta L(S)}{\delta S_{i,j}}&=\bm{A}+\bm{B}\nonumber\\
    &=\frac{1}{2}\left(\sum_{c}^{N^2} \sum_{d}^{N^2} S_{c,d} C(O)_{i,c}C(O)_{j,d}+\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{a,i}C(O)_{b,j}\right)\nonumber\\
    &=\frac{1}{2}\left(\sum_{\highlight{a}}^{N^2} \sum_{\highlight{b}}^{N^2} S_{\highlight{a,b}} C(O)_{i,\highlight{a}}C(O)_{j,\highlight{b}}+\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{a,i}C(O)_{b,j}\right)\label{eq:simplification_1}\\
    &=\frac{1}{2}\left(\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{\highlight{a,i}}C(O)_{j,b}+\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{a,i}C(O)_{\highlight{j,b}}\right)\label{eq:simplification_2}\\
    &=\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{a,i}C(O)_{j,b}
\end{align}
\autoref{eq:simplification_1} utilizes the independence of the 2 summations, equating the summation variables. \autoref{eq:simplification_2} utilizes the symmetric nature of $C(O)$, in order to have $a$ and $b$ along the same axes within all terms.

For example, $\frac{\delta L(S)}{\delta S_{2,3}}$ for \autoref{fig:superpositionShade} is calculated as follows,
\begin{align}
    \frac{\delta L(S)}{\delta S_{1,1}}&=\sum_{a}^{N^2} \sum_{b}^{N^2} S_{a,b} C(O)_{a,i}C(O)_{j,b}\nonumber\\
      &=\sum_{a}^{N^2} \sum_{b}^{N^2}
      \begin{bmatrix}
          1&0&0&0\\
          0&1&0&0\\
          0&0&0&1\\
          0&0&1&0
      \end{bmatrix}\odot
      \begin{bmatrix}
          1&1&1&1\\
          0&0&0&0\\
          2&2&2&2\\
          1&1&1&1
      \end{bmatrix}\odot
      \begin{bmatrix}
          1&2&0&1\\
          1&2&0&1\\
          1&2&0&1\\
          1&2&0&1
      \end{bmatrix}\label{eq:partial_example}\\
      &=\sum_{a}^{N^2} \sum_{b}^{N^2}
      \begin{bmatrix}
          1&0&0&0\\
          0&0&0&0\\
          0&0&0&2\\
          0&0&0&0
      \end{bmatrix}\nonumber\\
      &=3
\end{align}
$C(O)_{a,i}$ and $C(O)_{j,b}$ within \autoref{eq:partial_example} are repeated along the columns and rows respectively, due to $i$ and $j$ being constant \wrt{} $a$ and $b$ respectively. Computing $\bm{J}$ for \autoref{fig:superpositionShade} results in,
\begin{align}
    \bm{J}=\begin{bmatrix}
          5&5&3&3\\
          5&5&3&3\\
          3&3&5&5\\
          3&3&5&5
      \end{bmatrix}
    \label{eq:jacobian_example}
\end{align}

Now, a naive approach would be to apply gradient descent as follows:
\begin{align}
    S'=S-\alpha\bm{J}
    \label{eq:naive_descent}
\end{align}
But doing so would not take into the constraints involved within discrete optimization (that generalize to continuous optimization).

\subsection{Generalization of Discrete Constraints}%
\label{sub:constraints}

\subsubsection{Doubly Stochastic Matrices}%
\label{ssub:doubly_stochastic_matrices}
First of all, \emph{negative weights are nonsensical}, since weights should reflect the portion of the token located within a certain position.

In the discrete problem, within $X$ or $O$, every position has a \emph{single} unique token, and every token has a \emph{single} unique position. Generalizing this to superposition, note that all rows and columns should sum to 1 (e.g., \autoref{fig:superpositionShade}). Intuitively, a superposition grid divides the weight of each token value, into fragments whose weights sum to 1, with every position containing in total weight 1, albeit from fragments and not \emph{whole} tokens.

Following from these observations, $S$ is said to be doubly stochastic. That is, any matrix $A$ with only non-negative values and
\begin{align}
    \sum_i A_{i,j}=\sum_j A_{i,j}=1
\end{align}
is doubly stochastic.\cite{weissteinDoubly} Therefore, to enforce this constraint, the superposition $S$ must remain doubly stochastic after the gradient descent update, or else, the weights could be set to 0, causing a loss of  $0-c$.

\subsubsection{Sinkhorn-Knopp Algorithm}%
\label{ssub:sinkhorn_knopp_algorithm}
This section explains the algorithm used within \autoref{ssub:zero_line_sum_modified_jacobian}.

A well-known algorithm to convert any non-negative matrix\footnote{\label{non-negative-caveat}having at least 1 positive value in each row and column} into a doubly stochastic matrix is the Sinkhorn-Knopp algorithm (also named RAS).\cite{sinkhorn1967concerning} There is a proof\cite{borobia1998matrix} and several papers analyzing its convergence.\cite{chakrabarty2018better,knight2008sinkhorn} Nonetheless, the algorithm itself is simple: alternating the normalization of rows and columns of a matrix. Here, ``normalization'' is defined as forming a sum of 1, by dividing each element within each row or column by the sum of the row or column.

Let $K$ be an $n\times n$ non-negative matrix.\footnote{see \autoref{non-negative-caveat}} A single iteration of RAS is defined as follows:
\begin{align}
        K'=&\begin{bmatrix}
                (\Sigma_j^N K_{1,j})^{-1}\\
                &\ddots{}\\
                &&(\Sigma_j^N K_{N,j})^{-1}
        \end{bmatrix}K &&\text{normalizing rows}\nonumber\\
            \ras(K)=K''=K'&\begin{bmatrix}
                (\Sigma_i^N K'_{i,1})^{-1}\\
                &\ddots{}\\
                &&(\Sigma_i^N K'_{i,N})^{-1}
        \end{bmatrix}&&\text{normalizing columns}
\end{align}
The scaling matrices are diagonal (non-diagonal elements are $0$s). Let $\ras^n(K)$ indicate $n$ iterations of RAS on $K$, i.e., $\ras^2(K)=\ras(\ras(K))$.

\autoref{fig:ras_demo} demonstrates the effectiveness of RAS in normalizing randomly sampled matrices. Graphed on the y-axis is the error: the squared distance between the sums of the rows and columns, and 1, defined as,
\begin{align}
        E(X)=\sum^N_i\left(\left(\sum^N_jX_{i,j}\right)-1\right)^2+\sum^N_j\left(\left(\sum^N_iX_{i,j}\right)-1\right)^2
\end{align}

\begin{figure}[htpb]
        \centering
        \input{fig/ras.tex}
        \caption{Demonstration of RAS convergence: RAS applied on 5 randomly generated $N\times N$ matrices with $N=100$, sampled from a uniform distribution $[0,\frac{2}{N})$ (mean $\frac{1}{N}$). Note the logarithmic scale.}%
        \label{fig:ras_demo}
\end{figure}

An example of a single iteration of RAS:
\begin{align}
    K=&\begin{bmatrix}
        0&2&4\\
        1&3&5\\
        2&4&6
    \end{bmatrix}\nonumber\\
    K'=
    \begin{bmatrix}
        \frac{1}{6}&0&0\\
        0&\frac{1}{9} &0\\
        0&0&\frac{1}{12}
    \end{bmatrix}
     &\begin{bmatrix}
        0&2&4\\
        1&3&5\\
        2&4&6
    \end{bmatrix}=
    \begin{bmatrix}
        0&\frac{1}{3}&\frac{2}{3}\\[4pt]
        \frac{1}{9}&\frac{1}{3}&\frac{5}{9}\\[4pt]
        \frac{1}{6}&\frac{1}{3}&\frac{1}{2}
    \end{bmatrix}\nonumber\\
    \ras(K)=K''=
     &\begin{bmatrix}
         0&\frac{1}{3}&\frac{2}{3}\\[4pt]
         \frac{1}{9}&\frac{1}{3}&\frac{5}{9}\\[4pt]
        \frac{1}{6}&\frac{1}{3}&\frac{1}{2}
    \end{bmatrix}
    \begin{bmatrix}
         \frac{18}{5}&0&0\\
         0&1&0\\
         0&0&\frac{18}{31}
    \end{bmatrix}=
    \begin{bmatrix}
        0&\frac{1}{3}&\frac{12}{31}\\[4pt]
        \frac{2}{5}&\frac{1}{3}&\frac{10}{31}\\[4pt]
        \frac{3}{5}&\frac{1}{3}&\frac{9}{31}
    \end{bmatrix}
\end{align}

Note that the sums of rows and columns of $\ras(K)$ are \emph{closer} to 1 than $K$.

\subsubsection{Zero Line-Sum Modified Jacobian}%
\label{ssub:zero_line_sum_modified_jacobian}
To maintain the doubly stochastic nature of the superposition grid within gradient descent update (\autoref{eq:naive_descent}), $\bm{J}$ must be modified into a zero line-sum (ZLS) matrix.\cite{zeroLineSum} A ZLS matrix has all rows and columns summing to 0, that is, a matrix $A$ is ZLS if and only if
\begin{align}
    \sum_i A_{i,j}=\sum_j A_{i,j}=0
\end{align}
Intuitively, this means that the gradient update should not change the sum of the weights of any token value or within a position (to maintain a doubly stochastic $S$).

As stated within \cite{zeroLineSum}, a ZLS can be obtained by taking the difference of 2 doubly stochastic matrices.
\begin{proof}
    Let $A$ and $B$ be doubly stochastic matrices.
    \begin{align}
        \sum_i \left(A_{i,j}-B_{i,j}\right)= \sum_i A_{i,j}-\sum_i B_{i,j}=1-1=0\nonumber\\
        \sum_j \left(A_{i,j}-B_{i,j}\right)= \sum_j A_{i,j}-\sum_j B_{i,j}=1-1=0
    \end{align}
    Therefore, $A-B$ is ZLS.
\end{proof}

Hence, $\bm{J}$ can approximate a doubly stochastic matrix through the RAS algorithm,\footnote{the Jacobian of the loss function is non-negative because $S$ and $C(O)$ are non-negative, so RAS is applicable} and by subtracting it by another doubly stochastic matrix, a ZLS-$\bm{J}$ can be obtained. This second doubly stochastic matrix $D$ can be easily obtained by scaling a ones-matrix (matrix filled with ones). Let $D$ with dimensions $N^2\times N^2$ be defined as follows:
\begin{align}
    D_{i,j}&=\frac{1}{N^2}\nonumber\\
    \sum_i^{N^2} D_{i,j}&=\sum_j^{N^2} D_{i,j}=N^2\times \frac{1}{N^2}=1 &\text{$D$ is doubly stochastic}
    \label{eq:d_doubly_stochastic}
\end{align}
Because $D$ is uniform (all elements have identical values), the relative magnitudes of the elements within $\ras^n(\bm{J})$ \wrt{} other elements are preserved, i.e.,
\begin{align}
\ras^n(\bm{J})_{a,b}>\ras^n(\bm{J})_{c,d} \iff \ras^n(\bm{J})_{a,b}-D_{a,b}>\ras^n(\bm{J})_{c,d}-D_{c,d}
\end{align}
Let $\bm{J}'=\ras^n(\bm{J})-D$. For $\bm{J}$ in \autoref{eq:jacobian_example}, and $n=1$ it is computed as follows:
\begin{align}
    \bm{J}'&=\ras^{1}\left(
    \begin{bmatrix}
          5&5&3&3\\
          5&5&3&3\\
          3&3&5&5\\
          3&3&5&5
      \end{bmatrix}
    \right)-
    \frac{1}{4}\begin{bmatrix}
        1&1&1&1\\
        1&1&1&1\\
        1&1&1&1\\
        1&1&1&1
    \end{bmatrix}\nonumber\\
    &=\frac{1}{16}\begin{bmatrix}
          5&5&3&3\\
          5&5&3&3\\
          3&3&5&5\\
          3&3&5&5
    \end{bmatrix}
    -\frac{1}{4}
    \begin{bmatrix}
        1&1&1&1\\
        1&1&1&1\\
        1&1&1&1\\
        1&1&1&1
    \end{bmatrix}=\frac{1}{16}
    \begin{bmatrix}
        1&1&-1&-1\\
        1&1&-1&-1\\
        -1&-1&1&1\\
        -1&-1&1&1
    \end{bmatrix}
    \label{eq:jacobian_prime}
\end{align}

\subsubsection{Non-negative Matrices}%
\label{ssub:non_negative_matrices}
Assuming $S$ is doubly stochastic, and with ZLS-$\bm{J}'$, the result of the gradient descent update $S'=S-\alpha \bm{J}'$, has rows and columns summing to 1. For example, with $S$ and $\bm{J}$  from \autoref{fig:superpositionShade} and \autoref{eq:jacobian_prime} respectively, and $\alpha=1$,
\begin{align}
    S'&=S-\alpha \bm{J}'\label{eq:alpha_magic}\\
      &=\begin{bmatrix}
          1&0&0&0\\
          0&1&0&0\\
          0&0&0&1\\
          0&0&1&0
      \end{bmatrix}-
    \frac{1}{16}
    \begin{bmatrix}
        1&1&-1&-1\\
        1&1&-1&-1\\
        -1&-1&1&1\\
        -1&-1&1&1
    \end{bmatrix}=\frac{1}{16}
    \begin{bmatrix}
        15&-1&1&1\\
        -1&15&1&1\\
        1&1&-1&15\\
        1&1&15&-1
    \end{bmatrix}
    \label{eq:superposition_prime}
\end{align}

\emph{But $S'$ is not necessarily doubly stochastic, because $S'_{i,j}\geq 0$ is not guaranteed}, as shown in \autoref{eq:superposition_prime}. Given that $S'$ has dimension $N^2\times N^2$ the following procedure corrects negative entries while still maintaining the rows and columns summing to 1,
\begin{align}
 S''_{i,j}=
\begin{cases}
    (S'_{i,j}-\min{S'})\times \frac{1}{1-N^2 \min{S'}}, & \text{if $\exists a,b\quad S'_{a,b}<0$}\\
    S'_{i,j}, & \text{if $\nexists a,b\quad S'_{a,b}<0$}
\end{cases}
\label{eq:superposition_prime_prime}
\end{align}
$\min{S'}$ refers to the smallest value within $S'$.\footnote{$\exists a,b\quad S'_{a,b}<0$ means ``exists $a,b$ such that $S'_{a,b}$ is negative''} $(S'_{i,j}-\min{S'})$ removes negative entries, adding $(-N^2\min{S'})$ to the sums of the rows and columns; dividing by the new sum restores the doubly stochastic nature of $S'$. $S''$ for \autoref{eq:superposition_prime} is calculated as follows:
 \begin{align}
     S''&=\frac{1}{1+4(\frac{1}{16})}
     \left(
    \frac{1}{16}
    \begin{bmatrix}
        15&-1&1&1\\
        -1&15&1&1\\
        1&1&-1&15\\
        1&1&15&-1
    \end{bmatrix}
    +
     \frac{1}{16}
    \begin{bmatrix}
        1&1&1&1\\
        1&1&1&1\\
        1&1&1&1\\
        1&1&1&1
\end{bmatrix}\right)\nonumber\\
&=
\begin{bmatrix}
    \frac{4}{5}&0&\frac{1}{10}&\frac{1}{10}\\[4pt]
    0&\frac{4}{5}&\frac{1}{10}&\frac{1}{10}\\[4pt]
    \frac{1}{10}&\frac{1}{10}&0&\frac{4}{5}\\[4pt]
        \frac{1}{10}&\frac{1}{10}&\frac{4}{5}&0
\end{bmatrix}
\end{align}

With this, $S$ can be optimized by gradient descent.

\subsection{Generalization to Discrete Solutions}%
\label{sub:generalization_to_discrete_solutions}
Note that only the continuous representation $S$ has been optimized, and not the discrete solution $X$, which has been the goal of the essay. Expanding on the idea of quantum superposition within physics, by observing (or revealing) the position of an electron, the probability density function of the electron collapses into a single point. Within this essay, the probability density function has been represented by the weights of fragments within various positions. For simplicity, instead of sampling from a distribution, revealing a token value will place the entire weight of the token within the position of the fragment with the highest weight (the most likely position).

Taking this analogy even further, the tokens are \emph{entangled}. In physics, if two electrons are entangled, revealing the \emph{spin} of one electron instantaneously reveals the spin of the other: if one is found to be spin up, the other electron has spin down, and vice versa. The system within this essay involves $N$  ``electrons'' (tokens), and when the position of a token is revealed, fragments of other tokens within the same position are removed --- no two tokens should inhabit the same position, adhering to the constraints of discrete optimization. Also, the fragments of revealed tokens (columns of $S$) and fragments within revealed positions (rows of $S$) are not subject to optimization since they are \emph{fixed}.

Let $R$ be the set of indices (rows and columns) of revealed fragments, initially empty. The procedure for revealing a token is defined as follows:
\begin{align}
    R\leftarrow &R\cup \argmax_{m,n}S_{m,n}\nonumber\\
    \text{such that}\enspace &(\nexists a\enspace (a,m)\in R) \land (\nexists b\enspace (n,b)\in R)
    \label{eq:reveal_token}
\end{align}
$\argmax_{m,n}S_{m,n}$ returns the indices of the fragment with the highest weight that does not share a token value or token position with any already revealed fragment. If there are multiple fragments with the largest value, any fragment can be chosen. $R\cup \argmax_{m,n}S_{m,n}$ is the set of already revealed fragments including the newly revealed fragment, and $A\leftarrow B$ indicates a reassignment: the new value of $A$ is $B$.

Note that the previous procedure only works when $\argmax$ is defined, \emph{when there are tokens to reveal}, when  $|R|<N$, where $|R|$ denotes cardinality, (i.e., the number of indices within $R$).

For example, with
\begin{align}
S=\begin{bmatrix}
    5&4&1&0\\
    4&1&3&0\\
    3&2&0&0\\
    0&0&0&0
\end{bmatrix}
\end{align}
revealing all tokens sequentially will result in $R=\{(1,1),(2,3),(3,2),(4,4)\}$, revealed in that order. Coordinates can be restored into tokens by applying \autoref{eq:flattening_scheme}.

To only perform optimization on non-revealed token values and columns, the revealed rows and columns can be removed from $\bm{J}$, to obtain $\bm{J}_{cut}$ and $\bm{J}_{cut}'$ (with $D$ adjusted to have $D_{i,j}=\frac{1}{N^2-|R|}$), which can then be expanded to its original dimensions $N^2\times N^2$, by filling the revealed rows and columns with $0$s, effectively preventing optimization of revealed rows and columns.

For example, with $\bm{J}$ from \autoref{eq:jacobian_example} and $R=\{(1,1)\}$ and $n=3$,
\begin{align}
    \bm{J}_{cut}&=
    \begin{bmatrix}
        5&3&3\\
        3&5&5\\
        3&5&5
    \end{bmatrix}\nonumber\\
    \bm{J}_{cut}'&=\ras^n(\bm{J}_{cut})-D\label{eq:jacobian_cut}\\
         &=\begin{bmatrix}
            0.4884& 0.2558& 0.2558\\
            0.2558& 0.3721& 0.3721\\
            0.2558& 0.3721& 0.3721
         \end{bmatrix} -\frac{1}{3}
         \begin{bmatrix}
             1&1&1\\
             1&1&1\\
             1&1&1
         \end{bmatrix}\nonumber\\
         &\approx
         \begin{bmatrix}
             0.1551&-0.0775&-0.0775\\
             -0.0775&0.0389&0.0389\\
             -0.0775&0.0389&0.0389\\
         \end{bmatrix}\nonumber\\
    \bm{J}_{final}'&\approx
         \begin{bmatrix}
             0&0&0&0\\
             0&0.1551&-0.0775&-0.0775\\
             0&-0.0775&0.0389&0.0389\\
             0&-0.0775&0.0389&0.0389
         \end{bmatrix}
         \label{eq:jacobian_final}
\end{align}
After which the methodology in \autoref{ssub:non_negative_matrices} can be used with $\bm{J}_{final}'$ to obtain $S'$ and $S''$, since $\bm{J}_{final}'$ is still ZLS.

\subsection{Optimization Procedure}%
\label{sub:optimization_procedure}

\subsubsection{Learning Rate}%
\label{ssub:learning_rate}
$\alpha$ could simply be $1$. But note that as $N$ increases, the magnitude of each element within $\bm{J}'$ decreases due to RAS scaling ($N^2$ non-negative elements have to sum to 1) and ZLS offsetting. The following is an attempt to formulate a ``one size fits all'' solution for $\alpha$ for all $N$, without requiring human fine-tuning.
Recall \autoref{eq:alpha_magic}, now modified to accomodate revealed tokens,
\begin{align}
    S'=S-\alpha \bm{J}_{final}'
    \label{eq:alpha_magic_revealed}
\end{align}

Let $\Alpha$ be the set of possible values of $\alpha$ i.e., the $\alpha$ required to result in a 0-valued $S'_{i,j}$, \emph{only if $\bm{J}_{final_{i,j}}'$ is positive} (preventing $\alpha<0$ or a division by 0). That is,
\begin{align}
    S&=\alpha \bm{J}_{final}'\nonumber\\
    \Alpha&=\left\{\frac{S_{i,j}}{\bm{J}_{final_{i,j}}'}\bigg\rvert 1\leq i,j\leq N^2,\bm{J}_{final_{i,j}}'>0\right\}
\end{align}

$\alpha$ is then simply chosen to be the arithmetic mean of $\Alpha$. Intuitively, this process selects the $\alpha$ that removes about half of the fragments that contribute to the loss negatively, i.e., $\bm{J}_{final_{i,j}}'>0$). Furthermore, overshooting $\alpha$, causing $S'_{i,j}<0$, will be handled by \autoref{eq:superposition_prime_prime}.

For example, for $S$ from \autoref{fig:superpositionShade} and
$\bm{J}_{final_{i,j}}'$ from \autoref{eq:jacobian_final},
\begin{align}
    \Alpha&=\left\{
          \frac{1}{0.1551},\frac{0}{0.0389},\frac{1}{0.0389},\frac{1}{0.0389},\frac{0}{0.0389}
          \right\}\nonumber\\
          \alpha&\approx 11.57
          \label{eq:alpha_matrix}
\end{align}

\subsubsection{Superposition Initialization}%
\label{ssub:superposition_initialization}
Recall the optimization of $S$ requires $S$ to be doubly stochastic, and the steps in \autoref{sub:generalization_of_the_loss_function} have ensured the gradient descent update maintains the doubly stochastic nature of $S$, \emph{assuming $S$ is initially doubly stochastic.} This section aims to enforce that.

When no tokens have been revealed, $S=D$ (\autoref{eq:d_doubly_stochastic}). To work with an arbitrary amount of revealed tokens, $S$ with dimensions $N^2 \times N^2$ is defined as follows:
\begin{align}
S_{i,j}=
\begin{cases}
    \frac{1}{N^2-|R|},& \text{if $(\nexists a\enspace (a,j)\in R) \land (\nexists b\enspace (i,b)\in R)$\enspace if neither $i$ or $j$ have been revealed}\\
    0,& \text{if $(\exists a\enspace (a,j)\in R) \oplus (\exists b\enspace (i,b)\in R)$\enspace if only one of $i$ or $j$ has been revealed}\\
    1,& \text{if $(\exists a\enspace (a,j)\in R) \land (\exists b\enspace (i,b)\in R)$\enspace if $i,j$ is a revealed token}
\end{cases}
\label{eq:initialization_scheme}
\end{align}
Where $\land$ denotes a \emph{logical and} (true if both premises are true), and $\oplus$ is \emph{a logical XOR} (true if \emph{only 1} premise is true). For example, with $R=\{(3,4),(2,1)\}$ and $N=2$,
\begin{align}
    S=
    \begin{bmatrix}
        0&\frac{1}{2}&\frac{1}{2}&0\\
        1&0&0&0\\
        0&0&0&1\\
        0&\frac{1}{2}&\frac{1}{2}&0
    \end{bmatrix}
\end{align}

The doubly stochastic nature of $S$ is intuitive, rows and columns which are revealed have only 1 weight, 1, from the revealed token, and the weight of unrevealed rows and columns are shared between fragments without revealed rows and columns.

\subsubsection{Optimization Loop}%
\label{ssub:optimization_loop}
The following is a summary of the optimization procedure:
\begin{enumerate}
        \item For each element in $\{1,2,\ldots,N^2\}$ (revealing every token):
    \begin{enumerate}
    \item Initialize $S$ (\autoref{eq:initialization_scheme})
            \item For $n_{optim}$ steps:
    \begin{enumerate}
        \item Calculate $L(S)$ (\autoref{eq:final_loss})
        \item Calculate $\bm{J}$ (\autoref{eq:jacobian_definition}) and $\bm{J}_{cut}$ (\autoref{eq:jacobian_cut})
        \item Calculate $\alpha$ (\autoref{eq:alpha_matrix})
        \item Calculate ZLS-$\bm{J}_{cut}'$ (\autoref{eq:jacobian_prime}) and $\bm{J}_{final}'$ (\autoref{eq:jacobian_final})
        \item Obtain $S'$ from gradient descent update (\autoref{eq:alpha_magic_revealed})
        \item Ensure doubly stochastic $S''$ (\autoref{eq:superposition_prime_prime})
    \end{enumerate}
    \item Reveal token (\autoref{eq:reveal_token})
        \end{enumerate}
    \item Obtain final $X$ from $R$ using \autoref{eq:flattening_scheme}
\end{enumerate}
